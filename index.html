<!doctype html>
<html>

<head>
  <title>Cheng Han</title>
  <meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1">
  <link href="css/frame.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/controls.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/custom.css" media="screen" rel="stylesheet" type="text/css" />
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300,700' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="js/menu.js"></script>
  <script>
    (function (i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r;
      i[r] = i[r] || function () {
        (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date();
      a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
      a.async = 1;
      a.src = g;
      m.parentNode.insertBefore(a, m)
    })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
    ga('create', 'UA-103598896-1', 'auto');
    ga('send', 'pageview');
  </script>
</head>

<body>
  <div class="menu-container"></div>
  <div class="content-container">
    <div class="content">
      <div class="content-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <img class="image" id="me" src="img/han.jpg">
          </div>
          <div class="flex-item flex-column">
            <h2>Cheng Han</h2>
            <p class="text">
              PhD candidate<br>
              ch7858 (at) rit.com<br>
              <a href="https://www.rit.edu/" target="_blank">Rochester Institute of Technology, USA</a><br>
            </p>
          </div>
        </div>
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2>Biography</h2>
            <hr>
            <p class="text">
              I am a Ph.D. candidate from Chester F. Carlson Center for Imaging Science, Rochester Institute of Technology, USA.
              My research interests include computer vision, representation learning, and deep learning.
              <b>Specifically, my research is focused on parameter-efficient Artificial Intelligence (AI), where I design, implement, deploy, and evaluate energy-efficient AI systems that empower communities and address environmental and social issues.</b>
            </p>
            <h3>Table of Content</h3>
            <ul>
              <li><a href="#Publications">Publications</a></li>
              <li><a href="#Services">Services</a></li>
              <li><a href="#Talks and Presentation">Talks and Presentation</a></li>
              <!-- <li><a href="#journal-paper">Referred Journal and Magazine Papers</a></li>
              <li><a href="#conference-paper">Referred Conference Papers</a></li>
              <li><a href="#wip-paper">Referred Posters, Works-in-Progress, and Workshop Paper</a></li>
              <li><a href="#other-paper">Other Publications</a></li>
              <li><a href="#feature-media">Featured Media and Book Coverage</a></li>
              <li><a href="#other-media">Other Media and Book Coverage</a></li>
              <li><a href="#tool">Released Open Source Tools</a></li>
              <li><a href="#teaching">Teaching</a></li>
              <li><a href="#project">Projects</a></li> -->
            </ul>
            <h2 id="Publications">Publications</h2>
            <hr>
            <!--This list is reversed on the website due to reverse number listing-->
            <ol class="publication A-list">
              <li>
                <p class="text-small-margin">
                  Visual Recognition with Deep Nearest Centriods <a href="https://arxiv.org/abs/2209.07383" target="_blank">[pdf(arxiv)]</a><a href="https://github.com/ChengHan111/DNC" target="_blank">[code]</a>.
                  W. Wang*†, C. Han*, T. Zhou*, D. Liu†, International Conference on Learning Representations (ICLR), 2023 (Spotlight). *Equal Contribution
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  Prompt Learns Prompt: Exploring Knowledge-Aware Generative Prompt Collaboration For Video Captioning <a href="https://www.ijcai.org/proceedings/2023/0180.pdf" target="_blank">[pdf(IJCAI)]</a>.
                  L. Yan*, C. Han*, Z. Xu, D. Liu†, Q. Wang, International Joint Conferences on Artificial Intelligence (IJCAI), 2023. *Equal Contribution
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning <a href="https://arxiv.org/abs/2307.13770" target="_blank">[pdf(arxiv)]</a><a href="https://github.com/ChengHan111/E2VPT" target="_blank">[code]</a>.
                  C. Han*, Q. Wang, Y. Cui, Z. Cao, W. Wang, S. Qi, D. Liu†. International Conference on Computer Vision (ICCV), 2023.
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  Unified 3D Segmenter As Prototypical Classifiers <a href="https://openreview.net/pdf?id=Q6zd1hr7sD" target="_blank">[pdf(Openreview)]</a><a href="https://github.com/zyqin19/PROTOSEG" target="_blank">[code]</a>.
                  Z. Qin*, C. Han*, X. Lu, Q. Wang, X, Nie, Y, Yin†. Conference on Neural Information Processing Systems (NeurIPS), 2023. *Equal Contribution
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  CML-MOTS: Collaborative Multi-task Learning for Multi-Object Tracking and Segmentation <a href="https://arxiv.org/pdf/2311.00987" target="_blank">[pdf(arxiv)]</a>.
                  Y. Cui, C. Han, D. Liu†. ACM Journal on Autonomous Transportation Systems (JATS), 2023.
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  Facing the Elephant in the Room: Visual Prompt Tuning or Full Finetuning?
                  C. Han*, Q. Wang, Y. Cui, W. Wang, L. Huang, S. Qi, D. Liu†. International Conference on Learning Representations (ICLR), 2024 <a href="https://openreview.net/pdf?id=bJx4iOIOxn" target="_blank">[pdf(Openreview)]</a>.
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  Image Translation as Diffusion Visual Programmers.
                  C. Han, J. C. Liang, Q. Wang, M. Rabbani, S. Dianat, R. Rao, Y. N. Wu, D. Liu†. International Conference on Learning Representations (ICLR), 2024 <a href="https://openreview.net/pdf?id=yozwqhIHXj" target="_blank">[pdf(Openreview)]</a><a href="https://dvpmain.github.io/" target="_blank">[Website]</a>.
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  ProMotion: Prototypes As Motion Learners.
                  Y. Lu, D. Liu, Q., C. Han, Y. Cui, Z. Cao, X. Zhang, Y. V. Chen, H. Fan†. IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR), 2024.
                </p>
              </li>
            </ol>
            <h2 id="Services">Services</h2>
            <hr>
            <!--This list is reversed on the website due to reverse number listing-->
            <ol class="publication A-list">
              <li>
                <p class="text-small-margin">
                  <b>Program Committee</b>, The Association for the Advancement of Artificial Intelligence (AAAI), 2023.
                </p>
              <li>
              <li>
                <p class="text-small-margin">
                  <b>Reviewer</b>, The Conference on Computer Vision and Pattern Recognition (CVPR), 2024.
                </p>
              <li>
              <li>
                <p class="text-small-margin">
                  <b>Reviewer</b>, IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2022 -- Now.
                </p>
              <li>
              <li>
                <p class="text-small-margin">
                  <b>Reviewer</b>, Association for Computational Linguistics Rolling Review (ARR), 2023.
                </p>
              </li>
              <li>
                <p class="text-small-margin">
                  <b>Reviewer</b>, The International World Wide Web Conference (WWW), 2024.
                </p>
              </li>
            </ol>
            <h2 id="Talks and Presentation">Talks and Presentation</h2>
            <hr>
            <ol class="publication A-list">
              <li>
                <p class="text-small-margin">
                  <b> ICLR 2023 Spotlight Presentation</b>, Visual Recognition with Deep Nearest Centriods <a href="https://www.youtube.com/watch?v=NlzTniUXoqo" target="_blank">[YouTube]</a>.
                </p>
              <li>
              <li>
                <p class="text-small-margin">
                  <b> ICCV 2023 Presentation</b>, E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning <a href="https://www.youtube.com/watch?v=T6qOdypJ5Kc" target="_blank">[YouTube]</a>.
                </p>
              <li>
              <li>
                <p class="text-small-margin">
                  <b> NeurIPS 2023 Presentation</b>, Unified 3D Segmenter as Prototypical Classifiers. New Orleans, USA.
                </p>
              <li>
              <li>
                <p class="text-small-margin">
                  <b> ICLR 2024 Presentation</b>, Facing the Elephant in the Room: Visual Prompt Tuning or Full Finetuning?
                </p>
              <li>
              <li>
                <p class="text-small-margin">
                  <b> ICLR 2024 Presentation</b>, Image Translation as Diffusion Visual Programmers.
                </p>
              <li>
            </ol>
          <!--End Projects-->
        </div>
      </div>
    </div>
  </div>
</body>

</html>